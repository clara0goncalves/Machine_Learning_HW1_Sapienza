{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Install Dependecies"
      ],
      "metadata": {
        "id": "uYkosDKjN6-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "id": "bA3-dNGdN6VD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbfbb14e-c01a-48c4-8cd0-68b4c13d2c33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock (from torch)\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Collecting sympy (from torch)\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx (from torch)\n",
            "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch) (3.1.2)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m381.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.0.0 (from torch)\n",
            "  Downloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filelock-3.16.1 fsspec-2024.10.0 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.1.105 sympy-1.13.3 torch-2.4.1 triton-3.0.0 typing-extensions-4.12.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set device to GPU if is available otherwise set device as cpu**"
      ],
      "metadata": {
        "id": "83S06ig1RMKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check if GPU is available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "1IX6jLICPN4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfbe6b5a-e813-43ea-f6e7-49a574669e09"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "BaBjTEuIRHsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.base import BaseEstimator, RegressorMixin"
      ],
      "metadata": {
        "id": "7jsLAS7jRJ2-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Dataset##\n",
        "\n",
        "- The Datatset used in this project was generated using the Mujoco simulator with three different configurations:\n",
        "- 2D (2 joints)\n",
        "- 2D (3 joints)\n",
        "- 3D (5 joints)\n",
        "\n",
        "The format of the data is in CSV format, including information about Joint angles, fingertip position, and orientation.\n"
      ],
      "metadata": {
        "id": "hIO0AeMWIExL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1. Visualise data from the simulator"
      ],
      "metadata": {
        "id": "0_uqRtyhJDj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5 logfiler2.csv"
      ],
      "metadata": {
        "id": "6VflkHGKJDRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2e905e-8fce-4afb-8097-84f371038339"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j0;j1;cos(j0);cos(j1);sin(j0);sin(j1);ft_x;ft_y;ft_qw;ft_qz\r\n",
            " 0.055; -0.012;  0.998;  1.000;  0.055; -0.012;  0.210;  0.010;  1.000;  0.021\r\n",
            " 0.076; -0.017;  0.997;  1.000;  0.076; -0.017;  0.210;  0.014;  1.000;  0.030\r\n",
            " 0.148; -0.011;  0.989;  1.000;  0.147; -0.011;  0.208;  0.030;  0.998;  0.068\r\n",
            " 0.214;  0.048;  0.977;  0.999;  0.212;  0.048;  0.204;  0.050;  0.991;  0.131\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5 logfiler3.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNgZg0OJNaVV",
        "outputId": "ff24ddc0-a8af-4804-e97c-a0add4288be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j0;j1;j2;cos(j0);cos(j1);cos(j2);sin(j0);sin(j1);sin(j2);ft_x;ft_y;ft_qw;ft_qz\r\n",
            " 0.055; -0.012;  0.072;  0.998;  1.000;  0.997;  0.055; -0.012;  0.072;  0.309;  0.022;  0.998;  0.057\r\n",
            " 0.076; -0.017;  0.100;  0.997;  1.000;  0.995;  0.076; -0.017;  0.100;  0.308;  0.031;  0.997;  0.080\r\n",
            " 0.135; -0.059;  0.194;  0.991;  0.998;  0.981;  0.135; -0.059;  0.193;  0.305;  0.050;  0.991;  0.135\r\n",
            " 0.228; -0.110;  0.295;  0.974;  0.994;  0.957;  0.226; -0.109;  0.290;  0.297;  0.079;  0.979;  0.205\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5 logfiler5.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm_lpt6_Nc-X",
        "outputId": "c35fef0c-66e5-4c1a-ee7e-db3d1cb989b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j0;j1;j2;j3;j4;cos(j0);cos(j1);cos(j2);cos(j3);cos(j4);sin(j0);sin(j1);sin(j2);sin(j3);sin(j4);ft_x;ft_y;ft_z;ft_qw;ft_qx;ft_qy;ft_qz\r\n",
            " 0.000;  0.000;  0.000;  0.000;  0.000;  1.000;  1.000;  1.000;  1.000;  1.000;  0.000;  0.000;  0.000;  0.000;  0.000;  0.000;  0.000;  0.590;  1.000;  0.000;  0.000;  0.000\r\n",
            " 0.022; -0.005;  0.028;  0.016; -0.032;  1.000;  1.000;  1.000;  1.000;  0.999;  0.022; -0.005;  0.028;  0.016; -0.032;  0.011;  0.004;  0.590;  1.000; -0.016;  0.019;  0.011\r\n",
            " 0.103;  0.005;  0.107;  0.017; -0.100;  0.995;  1.000;  0.994;  1.000;  0.995;  0.102;  0.005;  0.106;  0.017; -0.099;  0.041;  0.016;  0.587;  0.995; -0.053;  0.061;  0.054\r\n",
            " 0.209;  0.067;  0.216;  0.013; -0.174;  0.978;  0.998;  0.977;  1.000;  0.985;  0.208;  0.067;  0.215;  0.013; -0.173;  0.100;  0.042;  0.573;  0.979; -0.101;  0.138;  0.116\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2. Preprocess the data\n"
      ],
      "metadata": {
        "id": "rbfjJsgxJI80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2R Robot"
      ],
      "metadata": {
        "id": "aCM-EXuFQxkU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dJxB_FilEapT"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv('logfiler2.csv', delimiter=';')\n",
        "\n",
        "# Preprocessing: Extract inputs (joint angles and their trigonometric functions) and outputs (fingertip positions and quaternions)\n",
        "X = data[['j0', 'j1', 'cos(j0)', 'cos(j1)', 'sin(j0)', 'sin(j1)']].values\n",
        "y = data[['ft_x', 'ft_y', 'ft_qw', 'ft_qz']].values\n",
        "\n",
        "# Normalize input features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Split the data into training and testing sets"
      ],
      "metadata": {
        "id": "a_UHnh5KJX6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "jze3VIDOQ_aq"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Train Forward Kinematics Models##\n",
        "\n"
      ],
      "metadata": {
        "id": "gcdL1pWYJNsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Robot 2R"
      ],
      "metadata": {
        "id": "LIaJ6J5pOJey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Define the architecture of the model (Feedforward Neural Network) to learn forward kinematics.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1yDUGGEzJSao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Define the loss function and optimizer"
      ],
      "metadata": {
        "id": "Tio9LYtzPKFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Neural Network model with Dropout\n",
        "class ForwardKinematicsModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ForwardKinematicsModel, self).__init__()\n",
        "        # Define a simple feedforward network with 2 hidden layers and Dropout\n",
        "        self.fc1 = nn.Linear(6, 64)  # Input layer (6 features)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)  # Dropout with 30% probability after first hidden layer\n",
        "        self.fc2 = nn.Linear(64, 64)       # Hidden layer\n",
        "        self.dropout2 = nn.Dropout(p=0.3)  # Dropout with 30% probability after second hidden layer\n",
        "        self.fc3 = nn.Linear(64, 4)        # Output layer (2 outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)  # Apply dropout after first hidden layer\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)  # Apply dropout after second hidden layer\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cxTQNHySMQM1"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hyperparameter Search"
      ],
      "metadata": {
        "id": "mvZc8OBWPvSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Grid Search"
      ],
      "metadata": {
        "id": "kEHlC7SSNV8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper class for PyTorch model to use with scikit-learn GridSearchCV\n",
        "class PyTorchRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, hidden_size=hidden_size, lr=learning_rate, epochs=100):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.model = ForwardKinematicsModel(hidden_size=self.hidden_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Convert data to torch tensors\n",
        "        X_train = torch.tensor(X, dtype=torch.float32)\n",
        "        y_train = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(X_train)\n",
        "            loss = self.criterion(output, y_train)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Convert data to torch tensor and return predictions\n",
        "        X_test = torch.tensor(X, dtype=torch.float32)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(X_test)\n",
        "        return predictions.numpy()\n",
        "\n",
        "# Load your dataset (replace this with your actual dataset)\n",
        "df = pd.read_csv('logfiler2.csv', sep=';')\n",
        "X = df[['j0', 'j1', 'cos(j0)', 'cos(j1)', 'sin(j0)', 'sin(j1)']].values  # Joint angles\n",
        "y = df[['ft_x', 'ft_y', 'ft_qw', 'ft_qz']].values  # Fingertip positions\n",
        "\n",
        "# Split dataset into training and test sets (80/20 split)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up hyperparameters to search\n",
        "param_grid = {\n",
        "    'hidden_size': [16, 32, 64],       # Number of hidden units in the hidden layers\n",
        "    'lr': [0.001, 0.01],            # Learning rates\n",
        "    'epochs': [100, 200, 500],           # Number of epochs to train\n",
        "}\n",
        "\n",
        "# Instantiate the PyTorch model wrapper for GridSearchCV\n",
        "pytorch_model = PyTorchRegressor()\n",
        "\n",
        "# Use GridSearchCV with the model wrapper\n",
        "grid_search = GridSearchCV(estimator=pytorch_model, param_grid=param_grid, cv=3, verbose=2, n_jobs=1)\n",
        "\n",
        "# Perform the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_loss = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Test MSE: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "Rn7gU9ofNU9l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "b02d09a7-4467-4a35-edd4-05ca2c00666b"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "__init__() got an unexpected keyword argument 'hidden_size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[201], line 52\u001b[0m\n\u001b[1;32m     45\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m],       \u001b[38;5;66;03m# Number of hidden units in the hidden layers\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m],            \u001b[38;5;66;03m# Learning rates\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m500\u001b[39m],           \u001b[38;5;66;03m# Number of epochs to train\u001b[39;00m\n\u001b[1;32m     49\u001b[0m }\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Instantiate the PyTorch model wrapper for GridSearchCV\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m pytorch_model \u001b[38;5;241m=\u001b[39m \u001b[43mPyTorchRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Use GridSearchCV with the model wrapper\u001b[39;00m\n\u001b[1;32m     55\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mpytorch_model, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "Cell \u001b[0;32mIn[201], line 7\u001b[0m, in \u001b[0;36mPyTorchRegressor.__init__\u001b[0;34m(self, hidden_size, lr, epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m lr\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mForwardKinematicsModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'hidden_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Train the models on joint angle inputs to predict fingertip positions."
      ],
      "metadata": {
        "id": "tDBMT_K2PiS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_epochs = 500\n",
        "learning_rate = 0.001\n",
        "hidden_size = 64\n",
        "\n",
        "# Initialize the model\n",
        "model = ForwardKinematicsModel()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "fz1ipjrTwD-y"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors only if necessary\n",
        "X_train = X_train.clone().detach().float() if isinstance(X_train, torch.Tensor) else torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = y_train.clone().detach().float() if isinstance(y_train, torch.Tensor) else torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val = X_val.clone().detach().float() if isinstance(X_val, torch.Tensor) else torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = y_val.clone().detach().float() if isinstance(y_val, torch.Tensor) else torch.tensor(y_val, dtype=torch.float32)\n",
        "X_test = X_test.clone().detach().float() if isinstance(X_test, torch.Tensor) else torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = y_test.clone().detach().float() if isinstance(y_test, torch.Tensor) else torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "szvcvhW6ylGh"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables for early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "no_improvement_epochs = 0\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs, y_val)\n",
        "\n",
        "            # Calculate additional metrics\n",
        "            mae = mean_absolute_error(y_val.numpy(), val_outputs.numpy())\n",
        "            r2 = r2_score(y_val.numpy(), val_outputs.numpy())\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "                  f\"Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
        "                  f\"MAE: {mae:.4f}, R^2: {r2:.4f}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                no_improvement_epochs = 0\n",
        "            else:\n",
        "                no_improvement_epochs += 1\n",
        "\n",
        "            if no_improvement_epochs >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
        "                break\n",
        "\n",
        "        model.train()  # Switch back to training mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "MxXXl4lG5FQh",
        "outputId": "2b21d77d-2df5-42c3-8993-83ea527208e0"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[203], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[200], line 13\u001b[0m, in \u001b[0;36mForwardKinematicsModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)  \u001b[38;5;66;03m# Apply dropout after first hidden layer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    test_loss = criterion(test_outputs, y_test)\n",
        "    mae_test = mean_absolute_error(y_test.numpy(), test_outputs.numpy())\n",
        "    r2_test = r2_score(y_test.numpy(), test_outputs.numpy())\n",
        "\n",
        "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
        "    print(f\"Test MAE: {mae_test:.4f}\")\n",
        "    print(f\"Test R^2: {r2_test:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXuwFfNlkmEQ",
        "outputId": "6dd9e65c-1cab-49d8-a0c6-046b09e80231"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0001\n",
            "Test MAE: 0.0076\n",
            "Test R^2: 0.9969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Compare Jacobians##\n"
      ],
      "metadata": {
        "id": "lWNe5FjVJolV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3.1. Compute the Jacobian matrix for the learned forward kinematics using automatic differentiation.\n",
        "\n"
      ],
      "metadata": {
        "id": "xTzh1e1BJu3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input: [j0, j1, cos(j0), cos(j1), sin(j0), sin(j1)]\n",
        "X_test = torch.tensor([[-3.097, 0.5, 0.3, 0.1, 0.4, 1.0]], dtype=torch.float32)  # 1 sample\n",
        "\n",
        "# Initialize the model\n",
        "model = ForwardKinematicsModel()\n",
        "\n",
        "# Define the FK and FK_Jacobian functions\n",
        "def FK(model, theta):\n",
        "    # Reshape to batch size 1\n",
        "    t = theta.view(1, -1)  # Ensure input is in the correct shape\n",
        "    out = model(t)\n",
        "    # Reshape to 1D vector (4D output for 4 outputs)\n",
        "    out = out.view(-1)\n",
        "    return out\n",
        "\n",
        "def FK_Jacobian(model, x):\n",
        "    # Enable gradient tracking for the input\n",
        "    x = x.requires_grad_(True)\n",
        "    y = FK(model, x)\n",
        "\n",
        "    # Initialize an empty list to store Jacobian rows\n",
        "    jacobian = []\n",
        "\n",
        "    # Compute the gradient of each output with respect to each input\n",
        "    for i in range(len(y)):  # len(y) = 4 (since we have 4 outputs)\n",
        "        grad_outputs = torch.zeros_like(y)\n",
        "        grad_outputs[i] = 1.0  # Create a vector with 1.0 at the i-th position\n",
        "\n",
        "        # Get the gradient of y[i] with respect to the inputs (j0, j1, etc.)\n",
        "        jacobian_row = torch.autograd.grad(y, x, grad_outputs=grad_outputs, retain_graph=True, create_graph=True)[0]\n",
        "\n",
        "        # For each output, we are interested in the gradient with respect to j0 and j1 (the first two inputs)\n",
        "        jacobian.append(jacobian_row[:2])  # Only keep gradients with respect to the first two inputs (j0, j1)\n",
        "\n",
        "    # Stack the rows to create a 4x2 Jacobian matrix\n",
        "    jacobian_matrix = torch.stack(jacobian, dim=0)  # This will give a 4x2 matrix\n",
        "    return jacobian_matrix\n",
        "\n",
        "# Use the FK and FK_Jacobian functions to compute the Jacobian\n",
        "jacobian = FK_Jacobian(model, X_test)\n",
        "print(\"Jacobian:\\n\", jacobian)"
      ],
      "metadata": {
        "id": "UYMolIGTqnMC",
        "outputId": "72dbc3dd-abd4-4905-9771-e8c54f02f390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jacobian:\n",
            " tensor([[[ 0.0338, -0.0060,  0.0148,  0.0256,  0.0068, -0.0353]],\n",
            "\n",
            "        [[-0.0208,  0.0027,  0.0047, -0.0187, -0.0611, -0.0080]],\n",
            "\n",
            "        [[ 0.0960, -0.1164, -0.0169,  0.0709,  0.1335, -0.0523]],\n",
            "\n",
            "        [[-0.0054,  0.0261, -0.1101, -0.0339,  0.0750,  0.0561]]],\n",
            "       grad_fn=<StackBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2. Compare the computed Jacobian with the analytical Jacobian for the 2-joint robot."
      ],
      "metadata": {
        "id": "rcFxsQjkJx1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Analytical Jacobian for a 2-DOF robot\n",
        "def analytical_jacobian_from_test(data):\n",
        "    # Extract joint angles from data\n",
        "    j0 = data[0]  # j0\n",
        "    j1 = data[1]  # j1\n",
        "    l1 = 1.0  # Link 1 length\n",
        "    l2 = 1.0  # Link 2 length\n",
        "\n",
        "    # Calculate the Jacobian elements using the analytical formula\n",
        "    J = np.array([\n",
        "        [-l1 * np.sin(j0) - l2 * np.sin(j0 + j1), -l2 * np.sin(j0 + j1)],\n",
        "        [l1 * np.cos(j0) + l2 * np.cos(j0 + j1), l2 * np.cos(j0 + j1)]\n",
        "    ])\n",
        "    return J\n",
        "\n",
        "# Example input from X_test[0]\n",
        "# j0, j1, cos(j0), cos(j1), sin(j0)\n",
        "X_test = X_test[0]\n",
        "# Compute analytical Jacobian\n",
        "J_analytical = analytical_jacobian_from_test(X_test_0)\n",
        "print(f\"Analytical Jacobian:\\n{J_analytical}\")"
      ],
      "metadata": {
        "id": "ZwQ2k3qMJ2Jp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cb9edd-20f4-4dd2-df52-36c2dccf16ef"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analytical Jacobian:\n",
            "[[-0.39535362 -0.29552021]\n",
            " [ 1.95034065  0.95533649]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iwykRT0xjfp-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}