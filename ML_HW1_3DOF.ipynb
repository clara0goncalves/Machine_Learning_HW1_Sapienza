{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Install Dependecies"
      ],
      "metadata": {
        "id": "uYkosDKjN6-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "id": "bA3-dNGdN6VD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c728492-99ab-4e03-f507-34b8ed518e3a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "BaBjTEuIRHsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.base import BaseEstimator, RegressorMixin"
      ],
      "metadata": {
        "id": "7jsLAS7jRJ2-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Random seed"
      ],
      "metadata": {
        "id": "yjSuYi7hlpEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)  # Python random module\n",
        "np.random.seed(seed)  # NumPy\n",
        "torch.manual_seed(seed)  # PyTorch\n",
        "\n",
        "# If you're using GPU, set the deterministic flag\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "oRmxPpCJloLq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set device to GPU if is available otherwise set device as cpu**"
      ],
      "metadata": {
        "id": "83S06ig1RMKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check if GPU is available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "1IX6jLICPN4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed36110a-34cd-41ad-acec-7827b4c0c1af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Dataset##\n",
        "\n",
        "- The Datatset used in this project was generated using the Mujoco simulator with three different configurations:\n",
        "- 2D (2 joints)\n",
        "- 2D (3 joints)\n",
        "- 3D (5 joints)\n",
        "\n",
        "The format of the data is in CSV format, including information about Joint angles, fingertip position, and orientation.\n",
        "\n",
        "This Notebook will focus on the 3DOF Robot dataset.\n"
      ],
      "metadata": {
        "id": "hIO0AeMWIExL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1. Visualise data from the simulator"
      ],
      "metadata": {
        "id": "0_uqRtyhJDj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5 logfiler3.csv"
      ],
      "metadata": {
        "id": "6VflkHGKJDRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda11b15-7f32-471a-faee-a2f55882c6f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j0;j1;j2;cos(j0);cos(j1);cos(j2);sin(j0);sin(j1);sin(j2);ft_x;ft_y;ft_qw;ft_qz\n",
            " 0.055; -0.012;  0.072;  0.998;  1.000;  0.997;  0.055; -0.012;  0.072;  0.309;  0.022;  0.998;  0.057\n",
            " 0.076; -0.017;  0.100;  0.997;  1.000;  0.995;  0.076; -0.017;  0.100;  0.308;  0.031;  0.997;  0.080\n",
            " 0.135; -0.059;  0.194;  0.991;  0.998;  0.981;  0.135; -0.059;  0.193;  0.305;  0.050;  0.991;  0.135\n",
            " 0.228; -0.110;  0.295;  0.974;  0.994;  0.957;  0.226; -0.109;  0.290;  0.297;  0.079;  0.979;  0.205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2. Preprocess the data\n"
      ],
      "metadata": {
        "id": "rbfjJsgxJI80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2R Robot"
      ],
      "metadata": {
        "id": "aCM-EXuFQxkU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dJxB_FilEapT"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv('logfiler3.csv', delimiter=';')\n",
        "\n",
        "# Preprocessing: Extract inputs (joint angles and their trigonometric functions) and outputs (fingertip positions and quaternions)\n",
        "X = data[['j0', 'j1', 'j2','cos(j0)', 'cos(j1)', 'cos(j2)', 'sin(j0)', 'sin(j1)', 'sin(j2)']].values\n",
        "y = data[['ft_x' ,'ft_y' ,'ft_qw','ft_qz']].values\n",
        "\n",
        "# Normalize input features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Split the data into training and testing sets"
      ],
      "metadata": {
        "id": "a_UHnh5KJX6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "jze3VIDOQ_aq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Train Forward Kinematics Models##\n",
        "\n"
      ],
      "metadata": {
        "id": "gcdL1pWYJNsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Robot 3DOF"
      ],
      "metadata": {
        "id": "LIaJ6J5pOJey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Define the architecture of the model (Feedforward Neural Network) to learn forward kinematics.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1yDUGGEzJSao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 250"
      ],
      "metadata": {
        "id": "fz1ipjrTwD-y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ForwardKinematicsModel(nn.Module):\n",
        "    def __init__(self, hidden_size, dropout_rate=0.3):\n",
        "        super(ForwardKinematicsModel, self).__init__()\n",
        "        # Define a feedforward network with configurable hidden_size and dropout\n",
        "        self.fc1 = nn.Linear(9, hidden_size)  # Input layer\n",
        "        self.dropout1 = nn.Dropout(p=dropout_rate)    # Dropout after first hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # Second hidden layer\n",
        "        self.dropout2 = nn.Dropout(p=dropout_rate)    # Dropout after second hidden layer\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)  # Output layer\n",
        "        self.dropout2 = nn.Dropout(p=dropout_rate)    # Dropout after second hidden layer\n",
        "        self.fc3 = nn.Linear(hidden_size, 4)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)  # Apply dropout after first hidden layer\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)  # Apply dropout after second hidden layer\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pwmiwxseZOMX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model with the best 2 parameters found by grid search"
      ],
      "metadata": {
        "id": "g9ueGIvGPcCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Train the models on joint angle inputs to predict fingertip positions."
      ],
      "metadata": {
        "id": "tDBMT_K2PiS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Define the loss function and optimizer"
      ],
      "metadata": {
        "id": "Tio9LYtzPKFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ForwardKinematicsModel(hidden_size=hidden_size)"
      ],
      "metadata": {
        "id": "3xDtcCCFO-hb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "j8l_q3FnzxFk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors only if necessary\n",
        "X_train = X_train.clone().detach().float() if isinstance(X_train, torch.Tensor) else torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = y_train.clone().detach().float() if isinstance(y_train, torch.Tensor) else torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val = X_val.clone().detach().float() if isinstance(X_val, torch.Tensor) else torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = y_val.clone().detach().float() if isinstance(y_val, torch.Tensor) else torch.tensor(y_val, dtype=torch.float32)\n",
        "X_test = X_test.clone().detach().float() if isinstance(X_test, torch.Tensor) else torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = y_test.clone().detach().float() if isinstance(y_test, torch.Tensor) else torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "szvcvhW6ylGh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables for early stopping and visualization\n",
        "best_val_loss = float('inf')\n",
        "patience = 4\n",
        "no_improvement_epochs = 0\n",
        "best_model_path = \"best_model_3DOF.pth\"  # Path to save the best model\n",
        "\n",
        "# Lists to store metrics for visualization\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_maes = []\n",
        "val_r2_scores = []\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store train loss for visualization\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val)\n",
        "            val_loss = criterion(val_outputs, y_val)\n",
        "            val_losses.append(val_loss.item())  # Store validation loss\n",
        "\n",
        "            # Calculate additional metrics\n",
        "            mae = mean_absolute_error(y_val.numpy(), val_outputs.numpy())\n",
        "            r2 = r2_score(y_val.numpy(), val_outputs.numpy())\n",
        "            val_maes.append(mae)  # Store validation MAE\n",
        "            val_r2_scores.append(r2)  # Store validation R^2\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "                  f\"Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
        "                  f\"MAE: {mae:.4f}, R^2: {r2:.4f}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                no_improvement_epochs = 0\n",
        "                # Save the best model\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                print(f\"Model saved at epoch {epoch+1} with Val Loss: {val_loss:.4f}\")\n",
        "            else:\n",
        "                no_improvement_epochs += 1\n",
        "\n",
        "            if no_improvement_epochs >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
        "                break  # Exit the training loop early\n",
        "\n",
        "        model.train()  # Switch back to training mode"
      ],
      "metadata": {
        "id": "fYuG0tz1nOAo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "737104dc-d3ae-4243-bbd0-8a4ded528ccc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/250], Train Loss: 0.0293, Val Loss: 0.0057, MAE: 0.0596, R^2: 0.9336\n",
            "Model saved at epoch 10 with Val Loss: 0.0057\n",
            "Epoch [20/250], Train Loss: 0.0290, Val Loss: 0.0055, MAE: 0.0586, R^2: 0.9351\n",
            "Model saved at epoch 20 with Val Loss: 0.0055\n",
            "Epoch [30/250], Train Loss: 0.0284, Val Loss: 0.0054, MAE: 0.0580, R^2: 0.9363\n",
            "Model saved at epoch 30 with Val Loss: 0.0054\n",
            "Epoch [40/250], Train Loss: 0.0280, Val Loss: 0.0053, MAE: 0.0576, R^2: 0.9376\n",
            "Model saved at epoch 40 with Val Loss: 0.0053\n",
            "Epoch [50/250], Train Loss: 0.0275, Val Loss: 0.0051, MAE: 0.0568, R^2: 0.9386\n",
            "Model saved at epoch 50 with Val Loss: 0.0051\n",
            "Epoch [60/250], Train Loss: 0.0271, Val Loss: 0.0050, MAE: 0.0563, R^2: 0.9398\n",
            "Model saved at epoch 60 with Val Loss: 0.0050\n",
            "Epoch [70/250], Train Loss: 0.0266, Val Loss: 0.0050, MAE: 0.0563, R^2: 0.9403\n",
            "Model saved at epoch 70 with Val Loss: 0.0050\n",
            "Epoch [80/250], Train Loss: 0.0262, Val Loss: 0.0049, MAE: 0.0554, R^2: 0.9414\n",
            "Model saved at epoch 80 with Val Loss: 0.0049\n",
            "Epoch [90/250], Train Loss: 0.0257, Val Loss: 0.0048, MAE: 0.0550, R^2: 0.9425\n",
            "Model saved at epoch 90 with Val Loss: 0.0048\n",
            "Epoch [100/250], Train Loss: 0.0254, Val Loss: 0.0047, MAE: 0.0547, R^2: 0.9432\n",
            "Model saved at epoch 100 with Val Loss: 0.0047\n",
            "Epoch [110/250], Train Loss: 0.0253, Val Loss: 0.0047, MAE: 0.0544, R^2: 0.9440\n",
            "Model saved at epoch 110 with Val Loss: 0.0047\n",
            "Epoch [120/250], Train Loss: 0.0252, Val Loss: 0.0046, MAE: 0.0536, R^2: 0.9450\n",
            "Model saved at epoch 120 with Val Loss: 0.0046\n",
            "Epoch [130/250], Train Loss: 0.0247, Val Loss: 0.0045, MAE: 0.0531, R^2: 0.9460\n",
            "Model saved at epoch 130 with Val Loss: 0.0045\n",
            "Epoch [140/250], Train Loss: 0.0242, Val Loss: 0.0045, MAE: 0.0531, R^2: 0.9464\n",
            "Model saved at epoch 140 with Val Loss: 0.0045\n",
            "Epoch [150/250], Train Loss: 0.0239, Val Loss: 0.0044, MAE: 0.0527, R^2: 0.9471\n",
            "Model saved at epoch 150 with Val Loss: 0.0044\n",
            "Epoch [160/250], Train Loss: 0.0236, Val Loss: 0.0044, MAE: 0.0525, R^2: 0.9481\n",
            "Model saved at epoch 160 with Val Loss: 0.0044\n",
            "Epoch [170/250], Train Loss: 0.0234, Val Loss: 0.0044, MAE: 0.0523, R^2: 0.9484\n",
            "Model saved at epoch 170 with Val Loss: 0.0044\n",
            "Epoch [180/250], Train Loss: 0.0230, Val Loss: 0.0043, MAE: 0.0518, R^2: 0.9491\n",
            "Model saved at epoch 180 with Val Loss: 0.0043\n",
            "Epoch [190/250], Train Loss: 0.0229, Val Loss: 0.0042, MAE: 0.0516, R^2: 0.9492\n",
            "Model saved at epoch 190 with Val Loss: 0.0042\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d12f2d390117>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
        "plt.plot(range(10, len(val_losses)*10 + 1, 10), val_losses, label=\"Validation Loss\", linestyle=\"--\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot validation MAE\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(range(10, len(val_maes)*10 + 1, 10), val_maes, label=\"Validation MAE\", color=\"orange\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"MAE\")\n",
        "plt.title(\"Validation Mean Absolute Error\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot validation R^2\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(range(10, len(val_r2_scores)*10 + 1, 10), val_r2_scores, label=\"Validation R^2\", color=\"green\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"R^2 Score\")\n",
        "plt.title(\"Validation R^2 Score\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zYBkC3cdnPyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    test_loss = criterion(test_outputs, y_test)\n",
        "    mae_test = mean_absolute_error(y_test.numpy(), test_outputs.numpy())\n",
        "    r2_test = r2_score(y_test.numpy(), test_outputs.numpy())\n",
        "\n",
        "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
        "    print(f\"Test MAE: {mae_test:.4f}\")\n",
        "    print(f\"Test R^2: {r2_test:.4f}\")\n"
      ],
      "metadata": {
        "id": "lXuwFfNlkmEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Compare Jacobians##\n"
      ],
      "metadata": {
        "id": "lWNe5FjVJolV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3.1. Compute the Jacobian matrix for the learned forward kinematics using automatic differentiation.\n",
        "\n"
      ],
      "metadata": {
        "id": "xTzh1e1BJu3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test.clone().detach().float() if isinstance(X_test, torch.Tensor) else torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = y_test.clone().detach().float() if isinstance(y_test, torch.Tensor) else torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "vWgtFZP5OCJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_test is a tensor and you want to extract the first sample\n",
        "X_test_sample = X_test[0].clone().detach().unsqueeze(0).float()  # Add batch dimension and detach\n",
        "print(X_test_sample)"
      ],
      "metadata": {
        "id": "wXM9zpl2OMPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model after training\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print(\"Best model loaded for further evaluation or inference.\")"
      ],
      "metadata": {
        "id": "iqxhWP_UQORR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_3dof_jacobian(model, inputs):\n",
        "    # Ensure input gradients are enabled\n",
        "    if not inputs.requires_grad:\n",
        "        inputs.requires_grad = True\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Select outputs of interest\n",
        "    ft_x, ft_y = outputs[0], outputs[1]\n",
        "    # If the model outputs include other relevant data (like qw, qz), you can add them too, but we are focusing on x and y position here.\n",
        "\n",
        "    # Initialize the 2x3 Jacobian matrix\n",
        "    jacobian = torch.zeros(2, 3)\n",
        "\n",
        "    # Compute gradients for ft_x\n",
        "    grad_ft_x = torch.autograd.grad(ft_x, inputs, retain_graph=True, create_graph=True)[0]\n",
        "    jacobian[0, 0] = grad_ft_x[0]  # ∂ft_x/∂j0\n",
        "    jacobian[0, 1] = grad_ft_x[1]  # ∂ft_x/∂j1\n",
        "    jacobian[0, 2] = grad_ft_x[2]  # ∂ft_x/∂j2\n",
        "\n",
        "    # Compute gradients for ft_y\n",
        "    grad_ft_y = torch.autograd.grad(ft_y, inputs, retain_graph=True, create_graph=True)[0]\n",
        "    jacobian[1, 0] = grad_ft_y[0]  # ∂ft_y/∂j0\n",
        "    jacobian[1, 1] = grad_ft_y[1]  # ∂ft_y/∂j1\n",
        "    jacobian[1, 2] = grad_ft_y[2]  # ∂ft_y/∂j2\n",
        "\n",
        "    return jacobian"
      ],
      "metadata": {
        "id": "yNbeiZ1mRJRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2. Compare the computed Jacobian with the analytical Jacobian for the 2-joint robot."
      ],
      "metadata": {
        "id": "rcFxsQjkJx1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analytical Jacobian for a 3-DOF robot\n",
        "def analytical_jacobian_from_test(data):\n",
        "    # Extract joint angles from data\n",
        "    j0 = data[0]  # j0\n",
        "    j1 = data[1]  # j1\n",
        "    j2 = data[2]  # j2\n",
        "\n",
        "    l1 = 1.0  # Link 1 length\n",
        "    l2 = 1.0  # Link 2 length\n",
        "    l3 = 1.0  # Link 3 length\n",
        "\n",
        "    # Calculate the Jacobian elements using the analytical formula\n",
        "    J = np.array([\n",
        "        [-l1 * np.sin(j0) - l2 * np.sin(j0 + j1) - l3 * np.sin(j0 + j1 + j2),\n",
        "         -l2 * np.sin(j0 + j1) - l3 * np.sin(j0 + j1 + j2),\n",
        "         -l3 * np.sin(j0 + j1 + j2)],\n",
        "\n",
        "        [l1 * np.cos(j0) + l2 * np.cos(j0 + j1) + l3 * np.cos(j0 + j1 + j2),\n",
        "         l2 * np.cos(j0 + j1) + l3 * np.cos(j0 + j1 + j2),\n",
        "         l3 * np.cos(j0 + j1 + j2)]\n",
        "    ])\n",
        "\n",
        "    return J"
      ],
      "metadata": {
        "id": "ZwQ2k3qMJ2Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compute Diference between jacobian one iteration"
      ],
      "metadata": {
        "id": "XRZiP84yU3Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#Numerical Jacobian (calculated by FK_Jacobian)\n",
        "X_test_sample = torch.tensor([-1.0430,  0.1984,  0.4028, -0.5862,  1.3897,  1.1648, -1.1416,  0.3033,0.6011], requires_grad=True)\n",
        "jacobian_numerical = compute_3dof_jacobian(model, X_test_sample)\n",
        "\n",
        "# Analytical Jacobian (calculated using analytical formula)X_test_sample = [-3.0970, -2.6260, -0.9990, -0.8700, -0.0450, -0.4930] # Extract joint angles for comparison\n",
        "X_test_sample = X_test_sample.detach().numpy()\n",
        "J_analytical = analytical_jacobian_from_test(X_test_sample)\n",
        "\n",
        "# Print both Jacobians\n",
        "print(\"Numerical Jacobian (PyTorch):\\n\", jacobian_numerical)\n",
        "print(\"Analytical Jacobian (2-DOF Robot):\\n\", J_analytical)\n",
        "\n",
        "# Compare both Jacobians by calculating the difference\n",
        "jacobian_difference = (jacobian_numerical.detach().numpy() - J_analytical)\n",
        "print(\"Difference between Numerical and Analytical Jacobians:\\n\", jacobian_difference)"
      ],
      "metadata": {
        "id": "uopqI4FVOPVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compute diferences Full test set"
      ],
      "metadata": {
        "id": "Zv6EgK8yU7PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store results\n",
        "numerical_jacobians = []\n",
        "analytical_jacobians = []\n",
        "jacobian_differences = []\n",
        "\n",
        "# Loop through the test set to compute the Jacobians and differences\n",
        "for i in range(len(X_test)):\n",
        "    # Get the current sample (convert to tensor and ensure requires_grad=True)\n",
        "    X_test_sample = torch.tensor(X_test[i], dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    # Compute the numerical Jacobian using the model\n",
        "    jacobian_numerical = compute_3dof_jacobian(model, X_test_sample)\n",
        "\n",
        "    # Compute the analytical Jacobian\n",
        "    X_test_sample_np = X_test_sample.detach().numpy()\n",
        "    J_analytical = analytical_jacobian_from_test(X_test_sample_np)\n",
        "\n",
        "    # Calculate the difference between the Jacobians\n",
        "    jacobian_difference = jacobian_numerical.detach().numpy() - J_analytical\n",
        "\n",
        "    # Store the results\n",
        "    numerical_jacobians.append(jacobian_numerical.detach().numpy())\n",
        "    analytical_jacobians.append(J_analytical)\n",
        "    jacobian_differences.append(jacobian_difference)\n",
        "\n",
        "# Convert lists to numpy arrays for analysis\n",
        "numerical_jacobians = np.array(numerical_jacobians)\n",
        "analytical_jacobians = np.array(analytical_jacobians)\n",
        "jacobian_differences = np.array(jacobian_differences)\n",
        "\n",
        "# Calculate the mean difference\n",
        "mean_difference = np.mean(np.abs(jacobian_differences), axis=(1, 2))  # Mean of absolute differences for each sample\n",
        "mean_difference_overall = np.mean(mean_difference)  # Overall mean difference\n",
        "\n",
        "# Calculate percentage of times analytical Jacobian is bigger or smaller\n",
        "analytical_bigger = np.sum(np.linalg.norm(analytical_jacobians, axis=(1, 2)) > np.linalg.norm(numerical_jacobians, axis=(1, 2)))\n",
        "analytical_smaller = np.sum(np.linalg.norm(analytical_jacobians, axis=(1, 2)) < np.linalg.norm(numerical_jacobians, axis=(1, 2)))\n",
        "total = len(X_test)\n",
        "\n",
        "percent_bigger = (analytical_bigger / total) * 100\n",
        "percent_smaller = (analytical_smaller / total) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean difference between numerical and analytical Jacobians: {mean_difference_overall:.6f}\")\n",
        "print(f\"Percentage of cases where analytical Jacobian is bigger: {percent_bigger:.2f}%\")\n",
        "print(f\"Percentage of cases where analytical Jacobian is smaller: {percent_smaller:.2f}%\")"
      ],
      "metadata": {
        "id": "k6jdskI_akz4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}